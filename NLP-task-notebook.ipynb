{"cells":[{"cell_type":"markdown","metadata":{"id":"EHn8xRsUtmhT"},"source":["# NLP Challenge Task\n","\n","This task contains a total of three problems. The weightage of three task is as follows:\n","\n","* Problem 1: 20%\n","* Problem 2: 30%\n","* Problem 3: 50%"]},{"cell_type":"markdown","metadata":{"id":"kqvOFR7HvBLm"},"source":["## Problem 1 TF-IDF\n","\n","Implement TF-IDF using using Python, Numpy, Pandas and whatever text cleaning library required.\n","\n","The tfâ€“idf is the product of two statistics, term frequency and inverse document frequency. There are various ways for determining the exact values of both statistics, you can use the following formulas.\n","\n","### Term Frequency\n","$$tf_{t,d} = \\log_{10}(count(t,d) +1)$$ \n","\n","* $tf_{t,d}$ is the frequency of the word t in the\n","document d\n","\n","### Inverse Document Frequency\n","$$idf_t = \\log_{10}(\\frac{N}{df_t})$$\n","\n","* $N$ is the total number of documents\n","* $df_t $ is the number of documents in which term t occurs\n","\n","### TF-IDF\n","$$tf\\text{-}idf_{t,d} = tf_{t,d} \\times idf_t $$\n","\n","### What is expected? \n","Your implementation should include the following two functions:\n"," * `compute_tfidf_weights(train_docs)`\n"," * `word_tfidf_vector(word, tf_df, idf_df)`\n"]},{"cell_type":"code","source":["!pip install numpy pandas nltk"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4fEM-X71W02B","executionInfo":{"status":"ok","timestamp":1684814683261,"user_tz":-420,"elapsed":7319,"user":{"displayName":"Tran Duc Anh Dang","userId":"03101702869062764274"}},"outputId":"2771484a-da1c-4ab9-b166-e51730687e47"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.22.4)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2022.7.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"zS2YXV_ciVHJ","executionInfo":{"status":"ok","timestamp":1684814687081,"user_tz":-420,"elapsed":3823,"user":{"displayName":"Tran Duc Anh Dang","userId":"03101702869062764274"}}},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from nltk.corpus import stopwords\n","from nltk import word_tokenize\n","from collections import Counter\n","import math\n","import itertools    \n","\n","def compute_tfidf_weights(train_docs):\n","  \"\"\"\n","  Computes the TF-IDF weights for a list of documents.\n","\n","  Args:\n","    train_docs: A list of documents, where each document is a string.\n","\n","  Returns:\n","    A tuple of (tf_df, idf_df), where tf_df is a DataFrame of term frequencies and idf_df is a DataFrame of inverse document frequencies.\n","  \"\"\"\n","\n","  # Consider English stopwords\n","  stop_words = set(stopwords.words('english'))\n","  \n","  # Tokenize documents, remove stopwords and count term frequencies\n","  tf_values = []\n","  df_counter = Counter()\n","\n","  for doc in train_docs:\n","    tokens = [word for word in word_tokenize(doc.lower()) if word.isalpha() and word not in stop_words]\n","    tf_values.append(Counter(tokens))\n","    df_counter.update(set(tokens))\n","\n","  # print(\"TF values: \", tf_values)\n","  # print(\"DF counter: \", df_counter)\n","\n","  # Convert term frequencies to tf scores\n","  tf_df = pd.DataFrame(tf_values).fillna(0)\n","  tf_df = tf_df.applymap(lambda x: math.log10(1 + x))\n","\n","  # print(\"TF DataFrame: \")\n","  # print(tf_df)\n","\n","  # Calculate idf scores\n","  N = len(train_docs)\n","  idf_scores = {word: math.log10(N / df_counter[word]) for word in df_counter.keys()}\n","  # idf_scores = {word: math.log10(N / (1 + df_counter[word])) for word in df_counter.keys()}\n","  # idf_scores = {word: math.log10((1 + N) / (1 + df_counter[word])) for word in df_counter.keys()}\n","\n","  idf_df = pd.DataFrame([idf_scores])\n","\n","  # print(\"IDF DataFrame: \")\n","  # print(idf_df)\n","\n","  return tf_df, idf_df\n","\n","def word_tfidf_vector(word, tf_df, idf_df):\n","  \"\"\"\n","  Calculates the TF-IDF vector for a word.\n","\n","  Args:\n","    word: A string.\n","    tf_df: A DataFrame of term frequencies.\n","    idf_df: A DataFrame of inverse document frequencies.\n","\n","  Returns:\n","    A numpy array of dimension 1xN, where N is the number of documents.\n","  \"\"\"\n","\n","  if word not in tf_df.columns:\n","        print(f\"The word '{word}' does not exist in the documents.\")\n","        return None\n","  \n","  # print(\"TF for the word:\")\n","  # print(tf_df[word])\n","  \n","  # print(\"IDF for the word:\")\n","  # print(idf_df[word].values[0])\n","\n","  tf_idf_value = tf_df[word] * idf_df[word].values[0]\n","  # print(\"TF-IDF values:\")\n","  # print(tf_idf_value)\n","\n","  return np.array(tf_idf_value)"]},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pSA509lFZ5gB","executionInfo":{"status":"ok","timestamp":1684814687081,"user_tz":-420,"elapsed":6,"user":{"displayName":"Tran Duc Anh Dang","userId":"03101702869062764274"}},"outputId":"8387ca5a-4d78-4142-a7c6-7a1474083486"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["# Test\n","\n","# Load the training data\n","train_docs = [\n","  \"This is a document about cats.\",\n","  \"This is another document about cats.\",\n","  \"This is a document about dogs.\",\n","]\n","\n","# Compute the TF-IDF weights\n","tf_df, idf_df = compute_tfidf_weights(train_docs)\n","\n","# Calculate the TF-IDF vector for the word \"cat\"\n","cat_tfidf_vector = word_tfidf_vector(\"cats\", tf_df, idf_df)\n","\n","# Print the TF-IDF vector\n","print(cat_tfidf_vector)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gV8rEV_bWqTN","executionInfo":{"status":"ok","timestamp":1684814687081,"user_tz":-420,"elapsed":5,"user":{"displayName":"Tran Duc Anh Dang","userId":"03101702869062764274"}},"outputId":"f9e9cee6-0498-4942-8ed9-520397458719"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.05300875 0.05300875 0.        ]\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Set the working directory for the tasks\n","import os\n","SKELETON_DIR = '/content/drive/MyDrive/NLP'\n","os.chdir(SKELETON_DIR)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"64oh8QFcggLB","executionInfo":{"status":"ok","timestamp":1684814710176,"user_tz":-420,"elapsed":23098,"user":{"displayName":"Tran Duc Anh Dang","userId":"03101702869062764274"}},"outputId":"88c33dc2-c039-481f-ee71-d1994ab81300"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","\n","# Load the CSV file\n","# df = pd.read_csv('/content/drive/MyDrive/NLP/Dataset/Corona_NLP_train.csv', encoding='latin1')\n","\n","p = 0.01\n","df = pd.read_csv('/content/drive/MyDrive/NLP/Dataset/Corona_NLP_train.csv', encoding='latin1', skiprows=lambda i: i>0 and np.random.random() > p)\n","\n","\n","# Preprocess the text data (this is a simple example, you might need to do more cleaning depending on your data)\n","df['OriginalTweet'] = df['OriginalTweet'].str.replace('http\\S+|www.\\S+', '', case=False) # remove URLs\n","\n","# Step 3: Calculate TF-IDF weights\n","tf_df, idf_df = compute_tfidf_weights(df['OriginalTweet'].tolist())\n","\n","# Calculate TF-IDF vector for a specific word\n","tf_idf_vector_word = word_tfidf_vector(\"coronavirus\", tf_df, idf_df) \n","print(tf_idf_vector_word)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xa5le7xPgzq8","executionInfo":{"status":"ok","timestamp":1684814715023,"user_tz":-420,"elapsed":4849,"user":{"displayName":"Tran Duc Anh Dang","userId":"03101702869062764274"}},"outputId":"7d4b1008-2e7c-4d8b-b64d-858709d0db74"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-6-ef64ba147e74>:12: FutureWarning: The default value of regex will change from True to False in a future version.\n","  df['OriginalTweet'] = df['OriginalTweet'].str.replace('http\\S+|www.\\S+', '', case=False) # remove URLs\n"]},{"output_type":"stream","name":"stdout","text":["[0.         0.11996046 0.11996046 0.         0.         0.11996046\n"," 0.19013283 0.         0.         0.         0.11996046 0.\n"," 0.11996046 0.         0.11996046 0.         0.         0.\n"," 0.11996046 0.11996046 0.         0.         0.11996046 0.\n"," 0.         0.         0.11996046 0.         0.11996046 0.\n"," 0.         0.11996046 0.11996046 0.11996046 0.         0.11996046\n"," 0.         0.11996046 0.         0.         0.         0.11996046\n"," 0.         0.         0.11996046 0.11996046 0.         0.11996046\n"," 0.11996046 0.         0.11996046 0.11996046 0.         0.\n"," 0.         0.         0.11996046 0.11996046 0.11996046 0.\n"," 0.         0.11996046 0.11996046 0.11996046 0.11996046 0.\n"," 0.         0.         0.11996046 0.11996046 0.         0.\n"," 0.11996046 0.         0.         0.         0.         0.\n"," 0.11996046 0.11996046 0.         0.         0.11996046 0.\n"," 0.         0.11996046 0.         0.         0.11996046 0.\n"," 0.11996046 0.11996046 0.11996046 0.11996046 0.         0.11996046\n"," 0.         0.11996046 0.         0.         0.11996046 0.11996046\n"," 0.         0.         0.11996046 0.         0.11996046 0.11996046\n"," 0.         0.11996046 0.         0.         0.11996046 0.\n"," 0.         0.         0.11996046 0.         0.         0.11996046\n"," 0.         0.         0.         0.         0.11996046 0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.11996046 0.11996046 0.         0.\n"," 0.         0.11996046 0.11996046 0.         0.         0.\n"," 0.         0.         0.11996046 0.11996046 0.         0.\n"," 0.         0.         0.         0.         0.11996046 0.\n"," 0.11996046 0.11996046 0.         0.11996046 0.11996046 0.11996046\n"," 0.11996046 0.11996046 0.         0.         0.11996046 0.\n"," 0.         0.         0.         0.11996046 0.         0.\n"," 0.11996046 0.19013283 0.11996046 0.11996046 0.11996046 0.\n"," 0.11996046 0.         0.         0.         0.19013283 0.11996046\n"," 0.11996046 0.         0.11996046 0.19013283 0.         0.\n"," 0.         0.19013283 0.         0.         0.         0.11996046\n"," 0.         0.11996046 0.11996046 0.         0.         0.\n"," 0.         0.11996046 0.11996046 0.         0.11996046 0.11996046\n"," 0.         0.11996046 0.11996046 0.         0.         0.\n"," 0.         0.11996046 0.11996046 0.11996046 0.         0.\n"," 0.11996046 0.11996046 0.         0.11996046 0.11996046 0.11996046\n"," 0.11996046 0.         0.         0.11996046 0.11996046 0.\n"," 0.11996046 0.         0.11996046 0.11996046 0.         0.\n"," 0.11996046 0.         0.         0.         0.11996046 0.\n"," 0.11996046 0.11996046 0.         0.         0.11996046 0.11996046\n"," 0.11996046 0.11996046 0.11996046 0.         0.         0.11996046\n"," 0.         0.11996046 0.11996046 0.         0.11996046 0.11996046\n"," 0.         0.         0.         0.         0.11996046 0.11996046\n"," 0.         0.         0.         0.11996046 0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.11996046 0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.11996046\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.19013283 0.11996046 0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.11996046 0.         0.11996046 0.11996046 0.11996046\n"," 0.         0.23992092 0.         0.11996046 0.11996046 0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.11996046 0.11996046 0.         0.11996046 0.11996046 0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.11996046 0.         0.         0.\n"," 0.         0.         0.         0.11996046 0.11996046 0.\n"," 0.11996046 0.         0.11996046 0.         0.         0.11996046\n"," 0.         0.11996046 0.11996046 0.         0.         0.\n"," 0.11996046 0.         0.         0.11996046 0.         0.11996046\n"," 0.         0.         0.         0.         0.         0.11996046\n"," 0.11996046 0.11996046 0.11996046 0.11996046 0.11996046 0.\n"," 0.         0.         0.         0.        ]\n"]}]},{"cell_type":"markdown","metadata":{"id":"cs5XDT4P-3py"},"source":["## Problem 2 POS for classification"]},{"cell_type":"markdown","metadata":{"id":"eyQIaUWB6tfR"},"source":["Robots and chat bots receive different commands to do certain tasks. \n","\n","Write a simple pragram that receive interactions in the form of a sentence and return:\n","* A tuple of (command, object) if the sentence is a command\n","* None if the sentence is not a command\n","\n","To write this function, you can utilize a Part-of-speech tagger or named-entity recognizer from libraries like NLTK and Spacy.\n","\n","Consider the following EXAMPLE sentences:\n","\n","* Commands:\n","  * Grab the book\n","  * Fetch the ball\n","  * Open the jar\n","  * Can hand this spoon to John?\n","\n","* Not commands:\n","  * Hey, how is it going?\n","  * How is your day today?\n","  * Do you like the weather?\n","This list is not exhaustive, your function should be able to handle more cases. \n","\n","Expected outcome:\n","\n","1. A function that performs the task\n","2. If your function has limitations, highlight those limitations with examples. You are not required to submit a different file. Write your answer in a 'Text' block in this notebook. "]},{"cell_type":"code","source":["!pip install spacy"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H6GTytWzoIHC","executionInfo":{"status":"ok","timestamp":1684814727575,"user_tz":-420,"elapsed":12555,"user":{"displayName":"Tran Duc Anh Dang","userId":"03101702869062764274"}},"outputId":"6e1872f8-1c2e-4b8e-de23-7ce543339874"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.5.2)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.4)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.9)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.7)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.8)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.9)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.1)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.6)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n","Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.7.0)\n","Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.1)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.3.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.65.0)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.22.4)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.27.1)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.7)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.1)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.2)\n"]}]},{"cell_type":"code","source":["import spacy\n","nlp = spacy.load('en_core_web_sm')\n","\n","def extract_command(sentence):\n","    \"\"\"\n","    This function takes a sentence as input and returns a command and object if they exist.\n","    Args:\n","    sentence (str): a string that may contain a command.\n","\n","    Returns:\n","    tuple: a tuple containing a command and object if they exist, otherwise None.\n","    \"\"\"\n","    doc = nlp(sentence)\n","    command_object_pairs = []\n","    for token in doc:\n","        if token.dep_ in {\"dobj\", \"pobj\"} and token.head.pos_ == \"VERB\":\n","            command_object_pairs.append((token.head.lemma_, token.text))\n","    if command_object_pairs:\n","        return command_object_pairs\n","    else:\n","        return None\n"],"metadata":{"id":"w5leCEjlob5m","executionInfo":{"status":"ok","timestamp":1684814748124,"user_tz":-420,"elapsed":20558,"user":{"displayName":"Tran Duc Anh Dang","userId":"03101702869062764274"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["import nltk\n","from nltk.tokenize import word_tokenize\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","\n","def extract_command_nltk(sentence):\n","    \"\"\"\n","    This function takes a sentence as input and returns a command and object if they exist.\n","    Args:\n","    sentence (str): a string that may contain a command.\n","\n","    Returns:\n","    tuple: a tuple containing a command and object if they exist, otherwise None.\n","    \"\"\"\n","    text = word_tokenize(sentence)\n","    pos_tags = nltk.pos_tag(text)\n","    \n","    command_object_pairs = []\n","    for i in range(len(pos_tags) - 1):\n","        if pos_tags[i][1].startswith('VB') and pos_tags[i+1][1] == 'DT': \n","            command = pos_tags[i][0]\n","            obj = pos_tags[i+1][0]\n","            command_object_pairs.append((command, obj))\n","            \n","    if command_object_pairs:\n","        return command_object_pairs\n","    else:\n","        return None\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HJrkVl-ppDTd","executionInfo":{"status":"ok","timestamp":1684814748125,"user_tz":-420,"elapsed":8,"user":{"displayName":"Tran Duc Anh Dang","userId":"03101702869062764274"}},"outputId":"b724e2c7-185c-4cf8-9ccf-65c704e5f89b"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"]}]},{"cell_type":"code","source":["sentences = [\"Grab the book\",\n","             \"Fetch the ball\",\n","             \"Open the jar\",\n","             \"Can you hand this spoon to John?\",\n","             \"Hey, how is it going?\",\n","             \"How is your day today?\",\n","             \"Do you like the weather?\"]\n","\n","print('Spacy: ')\n","for sentence in sentences:\n","    result = extract_command(sentence)\n","    if result:\n","        for pair in result:\n","            print(f\"Sentence: {sentence}\\nCommand: {pair[0]}, Object: {pair[1]}\\n\")\n","    else:\n","        print(f\"Sentence: {sentence}\\nThis sentence does not contain a command.\\n\")\n","\n","print('\\n\\n\\n\\nNLTK: ')\n","for sentence in sentences:\n","    result = extract_command_nltk(sentence)\n","    if result:\n","        for pair in result:\n","            print(f\"Sentence: {sentence}\\nCommand: {pair[0]}, Object: {pair[1]}\\n\")\n","    else:\n","        print(f\"Sentence: {sentence}\\nThis sentence does not contain a command.\\n\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dOerTKPfogAe","executionInfo":{"status":"ok","timestamp":1684814748591,"user_tz":-420,"elapsed":472,"user":{"displayName":"Tran Duc Anh Dang","userId":"03101702869062764274"}},"outputId":"9f3ede30-388a-4e70-c5d3-721f1c8b308d"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Spacy: \n","Sentence: Grab the book\n","Command: grab, Object: book\n","\n","Sentence: Fetch the ball\n","Command: fetch, Object: ball\n","\n","Sentence: Open the jar\n","Command: open, Object: jar\n","\n","Sentence: Can you hand this spoon to John?\n","Command: hand, Object: spoon\n","\n","Sentence: Hey, how is it going?\n","This sentence does not contain a command.\n","\n","Sentence: How is your day today?\n","This sentence does not contain a command.\n","\n","Sentence: Do you like the weather?\n","Command: like, Object: weather\n","\n","\n","\n","\n","\n","NLTK: \n","Sentence: Grab the book\n","Command: Grab, Object: the\n","\n","Sentence: Fetch the ball\n","Command: Fetch, Object: the\n","\n","Sentence: Open the jar\n","Command: Open, Object: the\n","\n","Sentence: Can you hand this spoon to John?\n","This sentence does not contain a command.\n","\n","Sentence: Hey, how is it going?\n","This sentence does not contain a command.\n","\n","Sentence: How is your day today?\n","This sentence does not contain a command.\n","\n","Sentence: Do you like the weather?\n","This sentence does not contain a command.\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"JN4SjrEYuFOx"},"source":["## Problem 3 Word embedding as features for classification\n","\n","### Task\n","Implement a sentiment classifier based on Twitter data to analyse the sentiments of COVID-19 tweets.  \n","\n","Train and test multiple classification model using necessary libraries with the features being sentence embeddings of tweets. \n","\n","Report the accuracy and F1 score (micro- and macro-averaged) for multiple classifier and discuss the differences. \n","\n","### Dataset\n","The dataset have been provided in Dataset.zip file. You are required to use the original tweet text for this classification task. \n","\n","### Tweet representation\n","After necessary pre-processing of the tweets, convert the words into their embeddings, then take the mean of all the word vectors in a tweet to end up with a single vector representing each tweet. The tweet vector is then used for sentiment classification.\n","\n","In the process of finding the embeddings for each word, you can ignore out-of-vocabulary words.\n","\n","### Embedding choice\n","For embedding, you can use GloVe embeddings using Gensim. A sample code is give below. \n","\n","However, this is a suggested option. You can use any word embedding of your choice, for example, word2vec, TF-IDF, etc., from any library of your choice.   \n","\n","### Classifier choice\n","You are required to implement the following classifiers: \n","* One tradition classification model (not a neural network based model)\n","* One classifier based on any neural network based model. \n","\n","You can use PyTorch/TensorFlow/scikit-learn to implement your classifier. However, you are free to develop a classifier from scratch. \n","\n","### Your answer must include the following: \n","1. Code for data loading, data pre-processing, training, and testing of the models.  \n","2. A discussion on the comparison between the classifiers based on classifier accuracy and F1 score.\n","\n","### Suggestion (Optional)\n","Consider saving a cleaned up version of the dataset after creating the embeddings to a file which can be loaded and used for further experimentation. "]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wp6BvFDzPWb-","outputId":"faf2f502-d411-4cb2-c180-f829d6568ea3","executionInfo":{"status":"ok","timestamp":1684814748910,"user_tz":-420,"elapsed":326,"user":{"displayName":"Tran Duc Anh Dang","userId":"03101702869062764274"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['fasttext-wiki-news-subwords-300',\n"," 'conceptnet-numberbatch-17-06-300',\n"," 'word2vec-ruscorpora-300',\n"," 'word2vec-google-news-300',\n"," 'glove-wiki-gigaword-50',\n"," 'glove-wiki-gigaword-100',\n"," 'glove-wiki-gigaword-200',\n"," 'glove-wiki-gigaword-300',\n"," 'glove-twitter-25',\n"," 'glove-twitter-50',\n"," 'glove-twitter-100',\n"," 'glove-twitter-200',\n"," '__testing_word2vec-matrix-synopsis']"]},"metadata":{},"execution_count":11}],"source":["import gensim.downloader as api\n","\n","list(api.info()['models'].keys())"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t0yOQhQvQGcO","outputId":"33aa924d-38ab-43b2-da84-7510685a76d9","executionInfo":{"status":"ok","timestamp":1684814815172,"user_tz":-420,"elapsed":66264,"user":{"displayName":"Tran Duc Anh Dang","userId":"03101702869062764274"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[==================================================] 100.0% 66.0/66.0MB downloaded\n"]},{"output_type":"execute_result","data":{"text/plain":["[('hydraulic', 0.8155338764190674),\n"," ('actuators', 0.7667093873023987),\n"," ('sprinkler', 0.7374147772789001),\n"," ('valve', 0.7271664142608643),\n"," ('actuation', 0.7141326665878296),\n"," ('hose', 0.7138993740081787),\n"," ('paddles', 0.7132106423377991),\n"," ('valves', 0.709661066532135),\n"," ('high-pressure', 0.7025710344314575),\n"," ('turntable', 0.7003635764122009)]"]},"metadata":{},"execution_count":12}],"source":["model = api.load(\"glove-wiki-gigaword-50\")\n","model.most_similar(\"pneumatic\")"]},{"cell_type":"code","source":["import gensim.downloader as api\n","import pandas as pd\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score, f1_score\n","from sklearn.neural_network import MLPClassifier\n","from nltk.tokenize import word_tokenize\n","import numpy as np\n","import nltk\n","\n","nltk.download('punkt')\n","\n","# Load data\n","p = 0.01\n","df_train = pd.read_csv('/content/drive/MyDrive/NLP/Dataset/Corona_NLP_train.csv', encoding='latin1') # skiprows=lambda i: i>0 and np.random.random() > p\n","tweets_train = df_train['OriginalTweet'].values\n","labels_train = df_train['Sentiment'].values\n","\n","# print(tweets_train)\n","# print(labels_train)\n","\n","df_test = pd.read_csv('/content/drive/MyDrive/NLP/Dataset/Corona_NLP_test.csv', encoding='latin1') # replace with your test dataset path\n","tweets_test = df_test['OriginalTweet'].values\n","labels_test = df_test['Sentiment'].values\n","\n","# print(tweets_test)\n","# print(labels_test)\n","\n","# Pre processing tweets\n","df_train['OriginalTweet'] = df_train['OriginalTweet'].str.replace('http\\S+|www.\\S+', '', case=False) # remove URLs\n","df_test['OriginalTweet'] = df_test['OriginalTweet'].str.replace('http\\S+|www.\\S+', '', case=False) # remove URLs\n","\n","\n","# Convert tweets into embedding\n","model = api.load(\"glove-wiki-gigaword-50\")\n","\n","def tweet_to_embedding(tweet):\n","    embeddings = []\n","    for word in word_tokenize(tweet):\n","        if word in model:\n","            embeddings.append(model[word])\n","    if embeddings:\n","        return np.mean(embeddings, axis=0)\n","    else:\n","        return np.zeros(model.vector_size)\n","\n","# Convert training tweets to embeddings\n","tweet_embeddings_train = np.array([tweet_to_embedding(tweet) for tweet in tweets_train])\n","tweet_embeddings_test = np.array([tweet_to_embedding(tweet) for tweet in tweets_test])\n","\n","\n","# Encode labels\n","le = LabelEncoder()\n","labels_train_encoded = le.fit_transform(labels_train)\n","labels_test_encoded = le.transform(labels_test)\n","\n","\n","# Traditional classifier (Random Forest)\n","clf_rf = RandomForestClassifier()\n","clf_rf.fit(tweet_embeddings_train, labels_train_encoded)\n","\n","# Neural network based classifier (MLP)\n","clf_mlp = MLPClassifier()\n","clf_mlp.fit(tweet_embeddings_train, labels_train_encoded)\n","\n","\n","# Test classifiers\n","y_pred_rf = clf_rf.predict(tweet_embeddings_test)\n","y_pred_mlp = clf_mlp.predict(tweet_embeddings_test)\n","\n","# Compare results\n","print(\"Random Forest Accuracy: \", accuracy_score(labels_test_encoded, y_pred_rf))\n","print(\"Random Forest F1 Score (micro): \", f1_score(labels_test_encoded, y_pred_rf, average='micro'))\n","print(\"Random Forest F1 Score (macro): \", f1_score(labels_test_encoded, y_pred_rf, average='macro'))\n","\n","print(\"MLP Accuracy: \", accuracy_score(labels_test_encoded, y_pred_mlp))\n","print(\"MLP F1 Score (micro): \", f1_score(labels_test_encoded, y_pred_mlp, average='micro'))\n","print(\"MLP F1 Score (macro): \", f1_score(labels_test_encoded, y_pred_mlp, average='macro'))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IKNpLcSktuE0","executionInfo":{"status":"ok","timestamp":1684815007623,"user_tz":-420,"elapsed":192468,"user":{"displayName":"Tran Duc Anh Dang","userId":"03101702869062764274"}},"outputId":"162743e5-d7d6-4a0e-9e26-c06f2ed1acd4"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","<ipython-input-13-200ec55d0eca>:30: FutureWarning: The default value of regex will change from True to False in a future version.\n","  df_train['OriginalTweet'] = df_train['OriginalTweet'].str.replace('http\\S+|www.\\S+', '', case=False) # remove URLs\n","<ipython-input-13-200ec55d0eca>:31: FutureWarning: The default value of regex will change from True to False in a future version.\n","  df_test['OriginalTweet'] = df_test['OriginalTweet'].str.replace('http\\S+|www.\\S+', '', case=False) # remove URLs\n","/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Random Forest Accuracy:  0.3417588204318062\n","Random Forest F1 Score (micro):  0.3417588204318062\n","Random Forest F1 Score (macro):  0.33127373269275534\n","MLP Accuracy:  0.3796735123749342\n","MLP F1 Score (micro):  0.3796735123749342\n","MLP F1 Score (macro):  0.38438828560258903\n"]}]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}